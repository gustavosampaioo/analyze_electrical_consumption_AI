import streamlit as st
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, 
                            recall_score, confusion_matrix, 
                            classification_report)
import matplotlib.pyplot as plt
import seaborn as sns
import google.generativeai as generai
from pathlib import Path
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import StandardScaler

# Configura√ß√£o inicial
st.set_page_config(page_title="An√°lise de Consumo de Energia", layout="wide")
st.title("üîç An√°lise de Consumo de Energia com Detec√ß√£o de Fraude")

# Configura√ß√£o da API do Google Generative AI
generai.configure(api_key="AIzaSyBHouRPqa8LLjU96nEPk6UJBgswH66OJjY")  # Substitua pela sua chave API

# Fun√ß√£o para encontrar arquivo no Desktop
def find_csv_file():
    desktop_paths = [
        Path.home() / "Desktop",
        Path.home() / "√Årea de Trabalho",
        Path.home() / "Escritorio",
    ]
    
    for desktop in desktop_paths:
        if desktop.exists():
            for file in desktop.glob("dados_consumo*.csv"):
                return str(file)
    return None

# Fun√ß√£o para carregar dados
def load_data(uploaded_file=None):
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            st.session_state['df'] = df
            return df
        except Exception as e:
            st.error(f"Erro ao ler arquivo: {str(e)}")
            return None
    
    auto_file = find_csv_file()
    if auto_file:
        try:
            df = pd.read_csv(auto_file)
            st.session_state['df'] = df
            st.success(f"Arquivo encontrado automaticamente: {auto_file}")
            return df
        except Exception as e:
            st.error(f"Erro ao ler arquivo autom√°tico: {str(e)}")
    
    st.warning("Nenhum arquivo encontrado. Por favor, fa√ßa upload do arquivo.")
    return None

# Fun√ß√£o para criar modelo neural
def create_nn_model(input_shape):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_shape,)),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])
    
    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy', 
                           tf.keras.metrics.Precision(),
                           tf.keras.metrics.Recall()])
    return model

# Fun√ß√£o para an√°lise com IA generativa
def analyze_with_ai(data_summary, metrics):
    model = generai.GenerativeModel("gemini-1.5-flash-latest")
    
    prompt = f"""
    Voc√™ √© um especialista em an√°lise de dados de consumo de energia e detec√ß√£o de fraudes. 
    Analise os seguintes dados e m√©tricas:

    **Resumo dos dados:**
    {data_summary}

    **M√©tricas do modelo:**
    {metrics}

    Sua an√°lise deve conter:
    1. Padr√µes de consumo de energia identificados
    2. Interpreta√ß√£o profissional das m√©tricas de avalia√ß√£o
    3. An√°lise da matriz de confus√£o
    4. Recomenda√ß√µes para melhorar a detec√ß√£o
    5. Per√≠odos de maior risco de fraude
    
    Formate a resposta em markdown com t√≠tulos e bullet points.
    """
    
    response = model.generate_content(prompt)
    return response.text

# Fun√ß√£o principal
def main():
    st.sidebar.header("Configura√ß√µes de Arquivo")
    
    # Op√ß√£o de upload
    uploaded_file = st.sidebar.file_uploader(
        "Carregar arquivo CSV", 
        type=["csv"],
        help="Selecione o arquivo dados_consumo.csv"
    )
    
    # Carrega os dados
    df = st.session_state.get('df', None)
    if uploaded_file or not df:
        df = load_data(uploaded_file)
    
    if df is not None:
        st.sidebar.header("Visualiza√ß√£o")
        show_raw_data = st.sidebar.checkbox("Mostrar dados brutos")
        
        if show_raw_data:
            st.subheader("Dados de Consumo")
            st.dataframe(df)
        
        # An√°lise explorat√≥ria
        st.subheader("üìä An√°lise Explorat√≥ria")
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Consumo M√©dio Di√°rio**")
            st.line_chart(df.set_index('data')['consumo_medio_diario'])
        
        with col2:
            st.write("**Consumo M√≠nimo Noturno**")
            st.bar_chart(df.set_index('data')['consumo_minimo_noturno'])
        
        # Pr√©-processamento avan√ßado
        features = df.iloc[:, 1:25]  # Colunas h1-h24
        target = df['status_fraude']
        
        # Normaliza√ß√£o dos dados
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # Divis√£o treino-valida√ß√£o-teste
        X_train, X_temp, y_train, y_temp = train_test_split(
            features_scaled, target, test_size=0.4, random_state=42
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42
        )
        
        # Treinar modelos
        st.subheader("ü§ñ Modelos de Detec√ß√£o")
        
        # Modelo Random Forest
        st.write("#### Random Forest")
        rf_model = RandomForestClassifier(random_state=42)
        rf_model.fit(X_train, y_train)
        y_pred_rf = rf_model.predict(X_test)
        
        # Modelo Neural Network
        st.write("#### Rede Neural")
        nn_model = create_nn_model(X_train.shape[1])
        history = nn_model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=20,
            batch_size=32,
            verbose=0
        )
        
        y_pred_nn = (nn_model.predict(X_test) > 0.5).astype(int)
        
        # Avalia√ß√£o dos modelos
        st.subheader("üìà M√©tricas de Avalia√ß√£o")
        
        # M√©tricas RF
        accuracy_rf = accuracy_score(y_test, y_pred_rf)
        precision_rf = precision_score(y_test, y_pred_rf)
        recall_rf = recall_score(y_test, y_pred_rf)
        cm_rf = confusion_matrix(y_test, y_pred_rf)
        
        # M√©tricas NN
        accuracy_nn = accuracy_score(y_test, y_pred_nn)
        precision_nn = precision_score(y_test, y_pred_nn)
        recall_nn = recall_score(y_test, y_pred_nn)
        cm_nn = confusion_matrix(y_test, y_pred_nn)
        
        # Exibi√ß√£o comparativa
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Random Forest**")
            st.metric("Acur√°cia", f"{accuracy_rf:.2%}")
            st.metric("Precis√£o", f"{precision_rf:.2%}")
            st.metric("Recall", f"{recall_rf:.2%}")
            
            st.write("**Matriz de Confus√£o**")
            fig, ax = plt.subplots()
            sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues',
                        xticklabels=['Normal', 'Fraude'],
                        yticklabels=['Normal', 'Fraude'])
            plt.ylabel('Verdadeiro')
            plt.xlabel('Predito')
            st.pyplot(fig)
        
        with col2:
            st.write("**Rede Neural**")
            st.metric("Acur√°cia", f"{accuracy_nn:.2%}")
            st.metric("Precis√£o", f"{precision_nn:.2%}")
            st.metric("Recall", f"{recall_nn:.2%}")
            
            st.write("**Matriz de Confus√£o**")
            fig, ax = plt.subplots()
            sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Greens',
                        xticklabels=['Normal', 'Fraude'],
                        yticklabels=['Normal', 'Fraude'])
            plt.ylabel('Verdadeiro')
            plt.xlabel('Predito')
            st.pyplot(fig)
        
        # Curvas de aprendizado
        st.subheader("üìö Curvas de Aprendizado (Rede Neural)")
        fig, ax = plt.subplots(1, 2, figsize=(15, 5))
        
        ax[0].plot(history.history['accuracy'], label='Treino')
        ax[0].plot(history.history['val_accuracy'], label='Valida√ß√£o')
        ax[0].set_title('Acur√°cia')
        ax[0].legend()
        
        ax[1].plot(history.history['loss'], label='Treino')
        ax[1].plot(history.history['val_loss'], label='Valida√ß√£o')
        ax[1].set_title('Loss')
        ax[1].legend()
        
        st.pyplot(fig)
        
        # An√°lise com IA generativa
        if st.button("üß† Obter An√°lise Avan√ßada com Gemini"):
            with st.spinner("Analisando dados com Gemini 1.5 Flash..."):
                try:
                    data_summary = df.describe().to_string()
                    
                    metrics = f"""
                    **Random Forest:**
                    - Acur√°cia: {accuracy_rf:.2%}
                    - Precis√£o: {precision_rf:.2%}
                    - Recall: {recall_rf:.2%}
                    
                    **Rede Neural:**
                    - Acur√°cia: {accuracy_nn:.2%}
                    - Precis√£o: {precision_nn:.2%}
                    - Recall: {recall_nn:.2%}
                    """
                    
                    analysis = analyze_with_ai(data_summary, metrics)
                    
                    st.subheader("üìù An√°lise com Gemini 1.5 Flash")
                    st.markdown(analysis)
                except Exception as e:
                    st.error(f"Erro na an√°lise com Gemini: {str(e)}")

if __name__ == "__main__":
    main()
