import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, 
                           recall_score, confusion_matrix, 
                           classification_report)
import matplotlib.pyplot as plt
import seaborn as sns

# Configura√ß√£o inicial
st.set_page_config(page_title="An√°lise de Consumo de Energia", layout="wide")
st.title("üîç An√°lise de Consumo de Energia com Detec√ß√£o de Fraude")

# Fun√ß√£o para encontrar arquivo no Desktop
def find_csv_file():
    desktop_paths = [
        Path.home() / "Desktop",
        Path.home() / "√Årea de Trabalho",
        Path.home() / "Escritorio",
    ]
    
    for desktop in desktop_paths:
        if desktop.exists():
            for file in desktop.glob("dados_consumo*.csv"):
                return str(file)
    return None

# Fun√ß√£o para carregar dados
def load_data(uploaded_file=None):
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            st.session_state['df'] = df
            return df
        except Exception as e:
            st.error(f"Erro ao ler arquivo: {str(e)}")
            return None
    
    auto_file = find_csv_file()
    if auto_file:
        try:
            df = pd.read_csv(auto_file)
            st.session_state['df'] = df
            st.success(f"Arquivo encontrado automaticamente: {auto_file}")
            return df
        except Exception as e:
            st.error(f"Erro ao ler arquivo autom√°tico: {str(e)}")
    
    st.warning("Nenhum arquivo encontrado. Por favor, fa√ßa upload do arquivo.")
    return None

# Fun√ß√£o para criar modelo neural (usando scikit-learn)
def create_nn_model():
    model = MLPClassifier(
        hidden_layer_sizes=(128, 64),
        activation='relu',
        solver='adam',
        learning_rate_init=0.001,
        max_iter=200,
        random_state=42,
        early_stopping=True,
        validation_fraction=0.2
    )
    return model

# Fun√ß√£o principal
def main():
    st.sidebar.header("Configura√ß√µes de Arquivo")
    
    # Op√ß√£o de upload
    uploaded_file = st.sidebar.file_uploader(
        "Carregar arquivo CSV", 
        type=["csv"],
        help="Selecione o arquivo dados_consumo.csv"
    )
    
    # Carrega os dados
    df = st.session_state.get('df', None)
    if uploaded_file or not df:
        df = load_data(uploaded_file)
    
    if df is not None:
        st.sidebar.header("Visualiza√ß√£o")
        show_raw_data = st.sidebar.checkbox("Mostrar dados brutos")
        
        if show_raw_data:
            st.subheader("Dados de Consumo")
            st.dataframe(df)
        
        # An√°lise explorat√≥ria
        st.subheader("üìä An√°lise Explorat√≥ria")
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Consumo M√©dio Di√°rio**")
            st.line_chart(df.set_index('data')['consumo_medio_diario'])
        
        with col2:
            st.write("**Consumo M√≠nimo Noturno**")
            st.bar_chart(df.set_index('data')['consumo_minimo_noturno'])
        
        # Pr√©-processamento avan√ßado
        features = df.iloc[:, 1:25]  # Colunas h1-h24
        target = df['status_fraude']
        
        # Normaliza√ß√£o dos dados
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # Divis√£o treino-valida√ß√£o-teste
        X_train, X_temp, y_train, y_temp = train_test_split(
            features_scaled, target, test_size=0.4, random_state=42
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42
        )
        
        # Treinar modelos
        st.subheader("ü§ñ Modelos de Detec√ß√£o")
        
        # Modelo Random Forest
        st.write("#### Random Forest")
        rf_model = RandomForestClassifier(random_state=42)
        rf_model.fit(X_train, y_train)
        y_pred_rf = rf_model.predict(X_test)
        
        # Modelo Neural Network (MLP)
        st.write("#### Rede Neural (MLP)")
        nn_model = create_nn_model()
        nn_model.fit(X_train, y_train)
        y_pred_nn = nn_model.predict(X_test)
        
        # Avalia√ß√£o dos modelos
        st.subheader("üìà M√©tricas de Avalia√ß√£o")
        
        # M√©tricas RF
        accuracy_rf = accuracy_score(y_test, y_pred_rf)
        precision_rf = precision_score(y_test, y_pred_rf)
        recall_rf = recall_score(y_test, y_pred_rf)
        cm_rf = confusion_matrix(y_test, y_pred_rf)
        
        # M√©tricas NN
        accuracy_nn = accuracy_score(y_test, y_pred_nn)
        precision_nn = precision_score(y_test, y_pred_nn)
        recall_nn = recall_score(y_test, y_pred_nn)
        cm_nn = confusion_matrix(y_test, y_pred_nn)
        
        # Exibi√ß√£o comparativa
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Random Forest**")
            st.metric("Acur√°cia", f"{accuracy_rf:.2%}")
            st.metric("Precis√£o", f"{precision_rf:.2%}")
            st.metric("Recall", f"{recall_rf:.2%}")
            
            st.write("**Matriz de Confus√£o**")
            fig, ax = plt.subplots()
            sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues',
                        xticklabels=['Normal', 'Fraude'],
                        yticklabels=['Normal', 'Fraude'])
            plt.ylabel('Verdadeiro')
            plt.xlabel('Predito')
            st.pyplot(fig)
        
        with col2:
            st.write("**Rede Neural (MLP)**")
            st.metric("Acur√°cia", f"{accuracy_nn:.2%}")
            st.metric("Precis√£o", f"{precision_nn:.2%}")
            st.metric("Recall", f"{recall_nn:.2%}")
            
            st.write("**Matriz de Confus√£o**")
            fig, ax = plt.subplots()
            sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Greens',
                        xticklabels=['Normal', 'Fraude'],
                        yticklabels=['Normal', 'Fraude'])
            plt.ylabel('Verdadeiro')
            plt.xlabel('Predito')
            st.pyplot(fig)
        
        # Se√ß√£o de interpreta√ß√£o
        st.subheader("üîç Guia de Interpreta√ß√£o")
        with st.expander("Como interpretar essas m√©tricas?"):
            st.markdown("""
            **Acur√°cia** (Accuracy):  
            > Porcentagem total de previs√µes corretas. √ötil para conjuntos balanceados.

            **Precis√£o** (Precision):  
            > Dos alertas de fraude emitidos, quantos eram realmente fraudes.

            **Recall** (Sensibilidade):  
            > Das fraudes reais existentes, quantas foram detectadas.

            **Matriz de Confus√£o**:
            - **TP** (True Positive): Fraudes detectadas corretamente
            - **FP** (False Positive): Consumos normais classificados como fraude
            - **TN** (True Negative): Consumos normais corretamente identificados
            - **FN** (False Negative): Fraudes n√£o detectadas
            """)

if __name__ == "__main__":
    main()
